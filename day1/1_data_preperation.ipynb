{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of ReadingData.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BDW5sY-dBnC"
      },
      "source": [
        "# Data preparation for a classification model\n",
        "\n",
        "In our first exercise, we will download the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 60,0000 small images (32 x 32 pixel) which fall into 10 different classes (automobile, airplane, ...).\n",
        "\n",
        "Here, we will learn how to visualize the data and prepare it for training a classifier model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2QTJEh7bgal"
      },
      "source": [
        "## Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnStabZOSK0O"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio import imread\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a7tUzIx-fMq8"
      },
      "source": [
        "# install a helper function to download and extract the cifar data\n",
        "!pip install cifar2png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K-UfXaQbvZX"
      },
      "source": [
        "## Download and read the data\n",
        "\n",
        "We download and load the images and corresponding labels to inspect them (and later use it to train a classifier).\n",
        "\n",
        "The data is organised in a folder structure as follows:\n",
        "\n",
        "```\n",
        "train/\n",
        "  airplane/\n",
        "    001.png\n",
        "    002.png\n",
        "    ...\n",
        "  automobile/\n",
        "    001.png\n",
        "    002.png\n",
        "    ...\n",
        "```\n",
        "\n",
        "and similarly for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKiPEN9K3L3G"
      },
      "source": [
        "cifar_dir = './cifar10'\n",
        "!cifar2png cifar10 cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MkRv0jlSQAv"
      },
      "source": [
        "# first, list the categories available in the data\n",
        "data_dir = os.path.join(cifar_dir, \"train\")\n",
        "categories = os.listdir(data_dir)\n",
        "categories.sort()\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVDpkJzUfMrW"
      },
      "source": [
        "# next load the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "for label_id, category in tqdm(enumerate(categories), total=len(categories)):\n",
        "    category_dir = os.path.join(data_dir, category)\n",
        "    image_names = os.listdir(category_dir)\n",
        "    for im_name in image_names:\n",
        "        im_file = os.path.join(category_dir, im_name)\n",
        "        images.append(imread(im_file))\n",
        "        labels.append(label_id)\n",
        "        \n",
        "# make numpy arrays out of the lists\n",
        "# for th images, we stack along a new first axis\n",
        "images = np.concatenate([im[None] for im in images], axis=0)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Number of images:\", len(images))\n",
        "print(\"Number of labels:\", len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgM21lmdya9D"
      },
      "source": [
        "# plot one image for each category\n",
        "fig, ax = plt.subplots(1, 10, figsize=(18, 6))\n",
        "label_list = labels.tolist()\n",
        "for label_id, category in enumerate(categories):\n",
        "    ax[label_id].imshow(images[label_list.index(label_id)])\n",
        "    # ax[label_id].set_title(category)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuoC2T1ofMrl"
      },
      "source": [
        "## Convolutional Filters\n",
        "\n",
        "As a first step we apply some simple filters on the images.\n",
        "In particular, we use convolutional filters, that can be expressed as convolution of a kernel with the image, which will be important for the concept of Convolutional Neural Networks that we will introduce later.\n",
        "\n",
        "For now, we will use some filter available in [skimage.filters](https://scikit-image.org/docs/dev/api/skimage.filters.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xDLjDG8fMrm"
      },
      "source": [
        "import skimage.filters as filters\n",
        "image = images[0]\n",
        "filtered_gaussian = filters.gaussian(image, sigma=1., multichannel=True)\n",
        "filtered_laplacian = filters.laplace(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3NwQDkCfMrs"
      },
      "source": [
        "fig, ax = plt.subplots(1, 3)\n",
        "ax[0].imshow(image)\n",
        "ax[1].imshow(filtered_gaussian)\n",
        "ax[2].imshow(filtered_laplacian)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MC9GmXgfMry"
      },
      "source": [
        "## Preparation for pytorch\n",
        "\n",
        "In order to use the CIFAR data to with pytorch, we need to transform the data into the compatible data structures. In particular, pytorch expects all numerical data as [torch.tensor](https://pytorch.org/docs/stable/tensors.html).\n",
        "To provide the data as tensors, we will wrap them in a [torch.dataset](https://pytorch.org/docs/stable/data.html) and implement a mechanism to apply transformations to the data on the fly. We will use these transformations to bring the data into a format that pytorch can ingest and later also use them for other purposes such as data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkDiA0BXfMr0"
      },
      "source": [
        "import torch\n",
        "# datasets have to be sub-classes from torch.util.data.Dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DatasetWithTransform(Dataset):\n",
        "    \"\"\" Our minimal dataset class. It holds data and target\n",
        "    as well as optional transforms that are applied to the data and target\n",
        "    on the fly when a batch is requested via the [] operator.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        assert isinstance(data, np.ndarray)\n",
        "        assert isinstance(target, np.ndarray)\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        if transform is not None:\n",
        "            assert callable(transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    # exposes the [] operator of our class\n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.data[index], self.target[index]\n",
        "\n",
        "        # if we have transformations, apply them to the data and target\n",
        "        if self.transform is not None:\n",
        "            data, target = self.transform(data, target)\n",
        "        return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-JWVCSfMr5"
      },
      "source": [
        "# what transofrmations do we need to feed this data to pytorch?\n",
        "\n",
        "# first, let's check the shape of our images:\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mId5K9urfMsA"
      },
      "source": [
        "# as we see, the images are stored in the order width, height, channel (WHC), \n",
        "# i.e. the first two axes are the image axes and the last axis\n",
        "# corresponds to the color channel.\n",
        "# pytorch however expects the color channel as first axis, i.e. CWH.\n",
        "# so our first transform switches the chanels\n",
        "\n",
        "# note that we have implemented the dataset in such a way, that the transforms\n",
        "# are functions that take bot the data (or image) and target as parameters.\n",
        "# thus we here accept the target (which is just the class label for the image) \n",
        "# as second parameter and return it without changing it\n",
        "def to_channel_first(image, target):\n",
        "    \"\"\" Transform images with color channel last (WHC) to channel first (CWH)\n",
        "    \"\"\"\n",
        "    # put channel first\n",
        "    image = image.transpose((2, 0, 1))\n",
        "    return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpUe36ExfMsH"
      },
      "source": [
        "# next, let's see what datatype and value range our images have\n",
        "print(image.dtype)\n",
        "print(image.min(), image.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XF1xLlpfMsP"
      },
      "source": [
        "# as we can see, the images are stored as 8 bit integers with a value range [0, 255]\n",
        "# instead, torch expects images as 32 bit floats that should also be normalized to a 'reasonable' data range.\n",
        "# here, we normalize the image such that all channels are in range 0 to 1\n",
        "def normalize(image, target, channel_wise=True):\n",
        "    eps = 1.e-6\n",
        "    image = image.astype('float32')\n",
        "    chan_min = image.min(axis=(1, 2), keepdims=True)\n",
        "    image -= chan_min\n",
        "    chan_max = image.max(axis=(1, 2), keepdims=True)\n",
        "    image /= (chan_max + eps)\n",
        "    return image, target\n",
        "\n",
        "\n",
        "# finally, we need to transform the input from a numpy array to a torch tensor\n",
        "# and also return the target (which in our case is a scalar) as a tensor\n",
        "def to_tensor(image, target):\n",
        "    return torch.from_numpy(image), torch.tensor([target], dtype=torch.int64)\n",
        "    \n",
        "\n",
        "# we also need a way to apply multiple transforms\n",
        "# (note that alternatively we could also have accepted a list of transforms\n",
        "# in DatasetWithTransform)\n",
        "def compose(image, target, transforms):\n",
        "    for trafo in transforms:\n",
        "        image, target = trafo(image, target)\n",
        "    return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDxwQd7mfMsU"
      },
      "source": [
        "# create the dataset with the transformations\n",
        "from functools import partial  # to bind function arguments\n",
        "\n",
        "trafos = [to_channel_first, normalize, to_tensor]\n",
        "trafo = partial(compose, transforms=trafos)\n",
        "\n",
        "dataset = DatasetWithTransform(images, labels, transform=trafo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcS3aC80fMsY"
      },
      "source": [
        "# function to show an image target pair returned from the dataset\n",
        "def show_image(ax, image, target):\n",
        "    # need to go back to numpy array and WHC axis order\n",
        "    image = image.numpy().transpose((1, 2, 0))\n",
        "    # find the label name\n",
        "    label = categories[target.item()]\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(label)\n",
        "\n",
        "    \n",
        "# sample a few images from the dataset and check their label\n",
        "n_images = len(dataset)\n",
        "n_samples = 8\n",
        "\n",
        "fig, ax = plt.subplots(1, n_samples, figsize=(18, 4))\n",
        "for sample in range(n_samples):\n",
        "    # datasets are random access, so we can request\n",
        "    # an image / target at an arbitrary index\n",
        "    sample_id = np.random.randint(0, n_images)\n",
        "    image, target = dataset[sample_id]\n",
        "    \n",
        "    # make sure that the image is in range 0, 1\n",
        "    assert np.isclose(image.min(), 0.)\n",
        "    assert np.isclose(image.max(), 1.)\n",
        "    \n",
        "    # add the image to our plots\n",
        "    show_image(ax[sample], image, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AWzMw6KfMsf"
      },
      "source": [
        "## Torchvision\n",
        "\n",
        "Note: there is a torch library for computer vision: [torchvision](https://pytorch.org/docs/stable/torchvision/index.html). It has many datasets, transformations etc. already. For example it also has a [prefab cifar dataset](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar).\n",
        "\n",
        "We do not use torchvision here for two reasons:\n",
        "- to show how to implement a torch.dataset yourself, so that you can implement it for new data you are working with\n",
        "- torchvision uses [PIL](https://pillow.readthedocs.io/en/stable/) to represent images, which is rather outdated\n",
        "\n",
        "Still, torchvision contains helpful functionality and many datasets, so it's a very useful library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZo-O66gfMse"
      },
      "source": [
        "## Tasks\n",
        "\n",
        "- Apply more advanced transforms in the dataset: for example you could blur the images or rotate them on the fly."
      ]
    }
  ]
}